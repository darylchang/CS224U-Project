on_ the_ emergence_n of_ rules_n in_ neural_a networks_n a_ simple_a associationist_n neural_a network_n learns_v to_ factor_v abstract_a rules_n (_ i.e._n ,_ grammars_ )_ from_ sequences_n of_ arbitrary_a input_n symbols_n by_ inventing_v abstract_a representations_n that_ accommodate_v unseen_a symbol_n sets_n as_r well_r as_ unseen_a but_ similar_a grammars_n ._ the_ neural_a network_n is_v shown_v to_ have_v the_ ability_n to_ transfer_v grammatical_a knowledge_n to_ both_ new_a symbol_n vocabularies_n and_ new_a grammars_n ._ analysis_n of_ the_ state-space_n shows_v that_ the_ network_n learns_v generalized_a abstract_a structures_n of_ the_ input_n and_ is_v not_r simply_r memorizing_v the_ input_n strings_n ._ these_ representations_n are_v context_n sensitive_a ,_ hierarchical_a ,_ and_ based_v on_ the_ state_n variable_a of_ the_ finite-state_n machines_n that_ the_ neural_a network_n has_v learned_v ._ generalization_n to_ new_a symbol_n sets_n or_ grammars_n arises_v from_ the_ spatial_a nature_n of_ the_ internal_a representations_n used_v by_ the_ network_n ,_ allowing_v new_a symbol_n sets_n to_ be_v encoded_v close_a to_ symbol_n sets_n that_ have_v already_r been_v learned_v in_ the_ hidden_a unit_n space_n of_ the_ network_n ._ the_ results_n are_v counter_r to_ the_ arguments_n that_ learning_v algorithms_n based_v on_ weight_n adaptation_n after_ each_ exemplar_n presentation_n (_ such_a as_ the_ long_a term_n potentiation_n found_v in_ the_ mammalian_n nervous_a system_n )_ can_ not_r in_ principle_n extract_v symbolic_a knowledge_n from_ positive_a examples_n as_ prescribed_a by_ prevailing_v human_a linguistic_a theory_n and_ evolutionary_a psychology_n