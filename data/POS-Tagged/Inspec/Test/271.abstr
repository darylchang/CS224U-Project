on_ the_ use_n of_ neural_a network_n ensembles_n in_ qsar_n and_ qspr_n despite_ their_ growing_v popularity_n among_ neural_a network_n practitioners_n ,_ ensemble_n methods_n have_v not_r been_v widely_r adopted_v in_ structure-activity_n and_ structure-property_n correlation_n ._ neural_a networks_n are_v inherently_r unstable_a ,_ in_ that_ small_a changes_n in_ the_ training_n set_v and/or_ training_n parameters_n can_ lead_v to_ large_a changes_n in_ their_ generalization_n performance_n ._ recent_a research_n has_v shown_v that_ by_ capitalizing_v on_ the_ diversity_n of_ the_ individual_a models_n ,_ ensemble_n techniques_n can_ minimize_v uncertainty_n and_ produce_v more_a stable_a and_ accurate_a predictors_n ._ in_ this_ work_n ,_ we_ present_v a_ critical_a assessment_n of_ the_ most_r common_a ensemble_n technique_n known_v as_ bootstrap_a aggregation_n ,_ or_ bagging_n ,_ as_ applied_v to_ qsar_n and_ qspr_n ._ although_ aggregation_n does_v offer_v definitive_a advantages_n ,_ we_ demonstrate_v that_ bagging_n may_ not_r be_v the_ best_a possible_a choice_n and_ that_ simpler_a techniques_n such_a as_ retraining_v with_ the_ full_a sample_n can_ often_r produce_v superior_a results_n ._ these_ findings_n are_v rationalized_v using_v krogh_n and_ vedelsby_n 's_ (_ 1995_ )_ decomposition_n of_ the_ generalization_n error_n into_ a_ term_n that_ measures_v the_ average_a generalization_n performance_n of_ the_ individual_a networks_n and_ a_ term_n that_ measures_v the_ diversity_n among_ them_ ._ for_ networks_n that_ are_v designed_v to_ resist_v over-fitting_a ,_ the_ benefits_n of_ aggregation_n are_v clear_a but_ not_r overwhelming_a